{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ekphrasis in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (0.5.1)\n",
      "Requirement already satisfied: tqdm in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (from ekphrasis) (4.31.1)\n",
      "Requirement already satisfied: termcolor in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (from ekphrasis) (1.1.0)\n",
      "Requirement already satisfied: colorama in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (from ekphrasis) (0.3.9)\n",
      "Requirement already satisfied: matplotlib in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (from ekphrasis) (2.2.2)\n",
      "Requirement already satisfied: ujson in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (from ekphrasis) (1.35)\n",
      "Requirement already satisfied: ftfy in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (from ekphrasis) (5.5.1)\n",
      "Requirement already satisfied: numpy in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (from ekphrasis) (1.16.2)\n",
      "Requirement already satisfied: nltk in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (from ekphrasis) (3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (from matplotlib->ekphrasis) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (from matplotlib->ekphrasis) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (from matplotlib->ekphrasis) (2.7.3)\n",
      "Requirement already satisfied: pytz in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (from matplotlib->ekphrasis) (2018.4)\n",
      "Requirement already satisfied: six>=1.10 in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (from matplotlib->ekphrasis) (1.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (from matplotlib->ekphrasis) (1.0.1)\n",
      "Requirement already satisfied: wcwidth in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (from ftfy->ekphrasis) (0.1.7)\n",
      "Requirement already satisfied: setuptools in /Users/PhilipADSo/anaconda3/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (39.1.0)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/PhilipADSo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/PhilipADSo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install ekphrasis\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import io, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "corpus = [\n",
    "     'This is the first test document is it better or fine!!!!??? https://www.google.com 10% £20 #chill :) :/',\n",
    "     'Playing I am happy to take your donation; any amount will be greatly appreciated.',\n",
    "     'Sometimes, all you need to do is completely make an ass of yourself and laugh it off to realise that life isn’t so bad after all.',\n",
    "     'I will never be this young again. Ever. Oh damn… I just got older.',\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "#Ekpharsis setup\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "   \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', 'first', 'test', 'document', 'is', 'it', 'better', 'or', 'fine', '?', '!', '<repeated>', '<url>', '<percent>', '<money>', '<hashtag>', 'chill', '</hashtag>', '<happy>', '<annoyed>']\n",
      "['playing', 'i', 'am', 'happy', 'to', 'take', 'your', 'donation', ';', 'any', 'amount', 'will', 'be', 'greatly', 'appreciated', '.']\n",
      "['sometimes', ',', 'all', 'you', 'need', 'to', 'do', 'is', 'completely', 'make', 'an', 'ass', 'of', 'yourself', 'and', 'laugh', 'it', 'off', 'to', 'realise', 'that', 'life', 'isn', '’', 't', 'so', 'bad', 'after', 'all', '.']\n",
      "['i', 'will', 'never', 'be', 'this', 'young', 'again', '.', 'ever', '.', 'oh', 'damn', '…', 'i', 'just', 'got', 'older', '.']\n"
     ]
    }
   ],
   "source": [
    "# Testing Ekpharsis\n",
    "tokenisedCorpus = []\n",
    "for sentence in corpus:\n",
    "    print(text_processor.pre_process_doc(sentence))\n",
    "    tokenisedCorpus.append(text_processor.pre_process_doc(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'test', 'document', 'better', 'fine', '?', '!', '<repeated>', '<url>', '<percent>', '<money>', '<hashtag>', 'chill', '</hashtag>', '<happy>', '<annoyed>']\n",
      "['playing', 'happy', 'take', 'donation', ';', 'amount', 'greatly', 'appreciated', '.']\n",
      "['sometimes', ',', 'need', 'completely', 'make', 'ass', 'laugh', 'realise', 'life', '’', 'bad', '.']\n",
      "['never', 'young', '.', 'ever', '.', 'oh', 'damn', '…', 'got', 'older', '.']\n"
     ]
    }
   ],
   "source": [
    "# Testing Stopwords\n",
    "for sentence in tokenisedCorpus:\n",
    "    print(remove_stopwords(sentence))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'be', 'the', 'first', 'test', 'document', 'be', 'it', 'better', 'or', 'fine', '?', '!', '<repeated>', '<url>', '<percent>', '<money>', '<hashtag>', 'chill', '</hashtag>', '<happy>', '<annoyed>']\n",
      "['play', 'i', 'be', 'happy', 'to', 'take', 'your', 'donation', ';', 'any', 'amount', 'will', 'be', 'greatly', 'appreciate', '.']\n",
      "['sometimes', ',', 'all', 'you', 'need', 'to', 'do', 'be', 'completely', 'make', 'an', 'ass', 'of', 'yourself', 'and', 'laugh', 'it', 'off', 'to', 'realise', 'that', 'life', 'isn', '’', 't', 'so', 'bad', 'after', 'all', '.']\n",
      "['i', 'will', 'never', 'be', 'this', 'young', 'again', '.', 'ever', '.', 'oh', 'damn', '…', 'i', 'just', 'get', 'older', '.']\n"
     ]
    }
   ],
   "source": [
    "# Testing Lemmatization\n",
    "for sentence in tokenisedCorpus:\n",
    "    print(lemmatize_verbs(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'is', 'the', 'first', 'test', 'docu', 'is', 'it', 'bet', 'or', 'fin', '?', '!', '<repeated>', '<url>', '<percent>', '<money>', '<hashtag>', 'chil', '</hashtag>', '<happy>', '<annoyed>']\n",
      "['play', 'i', 'am', 'happy', 'to', 'tak', 'yo', 'don', ';', 'any', 'amount', 'wil', 'be', 'gre', 'apprecy', '.']\n",
      "['sometim', ',', 'al', 'you', 'nee', 'to', 'do', 'is', 'complet', 'mak', 'an', 'ass', 'of', 'yourself', 'and', 'laugh', 'it', 'off', 'to', 'real', 'that', 'lif', 'isn', '’', 't', 'so', 'bad', 'aft', 'al', '.']\n",
      "['i', 'wil', 'nev', 'be', 'thi', 'young', 'again', '.', 'ev', '.', 'oh', 'damn', '…', 'i', 'just', 'got', 'old', '.']\n"
     ]
    }
   ],
   "source": [
    "# Testing Stemming\n",
    "for sentence in tokenisedCorpus:\n",
    "    print(stem_words(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data for over and undersampling\n",
    "\n",
    "label2emotion = {0:\"others\", 1:\"happy\", 2: \"sad\", 3:\"angry\"}\n",
    "emotion2label = {\"others\":0, \"happy\":1, \"sad\":2, \"angry\":3}\n",
    "\n",
    "def preprocessData(dataFilePath, mode):\n",
    "    \"\"\"Load data from a file, process and return indices, conversations and labels in separate lists\n",
    "    Input:\n",
    "        dataFilePath : Path to train/test file to be processed\n",
    "        mode : \"train\" mode returns labels. \"test\" mode doesn't return labels.\n",
    "    Output:\n",
    "        indices : Unique conversation ID list\n",
    "        conversations : List of 3 turn conversations, processed and each turn separated by the <eos> tag\n",
    "        labels : [Only available in \"train\" mode] List of labels\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    conversations = []\n",
    "    labels = []\n",
    "    with io.open(dataFilePath, encoding=\"utf8\") as finput:\n",
    "        finput.readline()\n",
    "        for line in finput:\n",
    "            # Convert multiple instances of . ? ! , to single instance\n",
    "            # okay...sure -> okay . sure\n",
    "            # okay???sure -> okay ? sure\n",
    "            # Add whitespace around such punctuation\n",
    "            # okay!sure -> okay ! sure\n",
    "            repeatedChars = ['.', '?', '!', ',']\n",
    "            for c in repeatedChars:\n",
    "                lineSplit = line.split(c)\n",
    "                while True:\n",
    "                    try:\n",
    "                        lineSplit.remove('')\n",
    "                    except:\n",
    "                        break\n",
    "                cSpace = ' ' + c + ' '    \n",
    "                line = cSpace.join(lineSplit)\n",
    "            \n",
    "            line = line.strip().split('\\t')\n",
    "            if mode == \"train\":\n",
    "                # Train data contains id, 3 turns and label\n",
    "                label = emotion2label[line[4]]\n",
    "                labels.append(label)\n",
    "            \n",
    "            conv = ' <eos> '.join(line[1:4])\n",
    "            \n",
    "            # Remove any duplicate spaces\n",
    "            duplicateSpacePattern = re.compile(r'\\ +')\n",
    "            conv = re.sub(duplicateSpacePattern, ' ', conv)\n",
    "            \n",
    "            indices.append(int(line[0]))\n",
    "            conversations.append(conv.lower())\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        return indices, conversations, labels\n",
    "    else:\n",
    "        return indices, conversations\n",
    "    \n",
    "trainIndices, trainTexts, labels = preprocessData(\"train.txt\", mode=\"train\")\n",
    "testIndices, testTexts, testLabels = preprocessData(\"test.txt\", mode=\"train\")\n",
    "devIndices, devTexts, devLabels = preprocessData(\"test.txt\", mode=\"train\")\n",
    "\n",
    "trainTexts += testTexts + devTexts\n",
    "labels+= testLabels + devLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 24302, 3: 6102, 2: 5963, 1: 4811})\n",
      "Resampled dataset shape Counter({0: 24302, 3: 24302, 2: 24302, 1: 24302})\n"
     ]
    }
   ],
   "source": [
    "# Testing Over sampling\n",
    "trainDF = pd.DataFrame({'convtrain': trainTexts,'labels': labels})\n",
    "from collections import Counter\n",
    "\n",
    "resampleTypeData = np.array(trainDF['convtrain'].values.tolist())\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "resampleTypeData = resampleTypeData.reshape(-1, 1)\n",
    "print('Original dataset shape %s' % Counter(trainDF['labels']))\n",
    "X_res, y_res = ros.fit_resample(resampleTypeData, trainDF['labels'])\n",
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 24302, 3: 6102, 2: 5963, 1: 4811})\n",
      "Resampled dataset shape Counter({0: 4811, 1: 4811, 2: 4811, 3: 4811})\n"
     ]
    }
   ],
   "source": [
    "# Testing Undersampling\n",
    "trainDF = pd.DataFrame({'convtrain': trainTexts,'labels': labels})\n",
    "from collections import Counter\n",
    "\n",
    "resampleTypeData = np.array(trainDF['convtrain'].values.tolist())\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "resampleTypeData = resampleTypeData.reshape(-1, 1)\n",
    "print('Original dataset shape %s' % Counter(trainDF['labels']))\n",
    "X_res, y_res = rus.fit_resample(resampleTypeData, trainDF['labels'])\n",
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 25)\t1\n",
      "  (0, 55)\t1\n",
      "  (0, 29)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 43)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 32)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 49)\t1\n",
      "  (0, 24)\t1\n",
      "  (0, 51)\t1\n",
      "  (0, 30)\t2\n",
      "  (0, 52)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 27)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 54)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 21)\t1\n",
      "  (1, 58)\t1\n",
      "  :\t:\n",
      "  (2, 39)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 36)\t1\n",
      "  (2, 17)\t1\n",
      "  (2, 19)\t1\n",
      "  (2, 37)\t1\n",
      "  (2, 56)\t1\n",
      "  (2, 4)\t2\n",
      "  (2, 47)\t1\n",
      "  (2, 53)\t2\n",
      "  (2, 32)\t1\n",
      "  (2, 30)\t1\n",
      "  (3, 42)\t1\n",
      "  (3, 26)\t1\n",
      "  (3, 33)\t1\n",
      "  (3, 18)\t1\n",
      "  (3, 41)\t1\n",
      "  (3, 22)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 57)\t1\n",
      "  (3, 38)\t1\n",
      "  (3, 13)\t1\n",
      "  (3, 54)\t1\n",
      "  (3, 52)\t1\n",
      "['10', '20', 'after', 'again', 'all', 'am', 'amount', 'an', 'and', 'any', 'appreciated', 'ass', 'bad', 'be', 'better', 'chill', 'com', 'completely', 'damn', 'do', 'document', 'donation', 'ever', 'fine', 'first', 'google', 'got', 'greatly', 'happy', 'https', 'is', 'isn', 'it', 'just', 'laugh', 'life', 'make', 'need', 'never', 'of', 'off', 'oh', 'older', 'or', 'playing', 'realise', 'so', 'sometimes', 'take', 'test', 'that', 'the', 'this', 'to', 'will', 'www', 'you', 'young', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "# Testing TF-IDF\n",
    "vectorizer = CountVectorizer()\n",
    "print(vectorizer.fit_transform(corpus))\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 52)\t0.18724231956240409\n",
      "  (0, 30)\t0.37448463912480817\n",
      "  (0, 51)\t0.23749314013993011\n",
      "  (0, 24)\t0.23749314013993011\n",
      "  (0, 49)\t0.23749314013993011\n",
      "  (0, 20)\t0.23749314013993011\n",
      "  (0, 32)\t0.18724231956240409\n",
      "  (0, 14)\t0.23749314013993011\n",
      "  (0, 43)\t0.23749314013993011\n",
      "  (0, 23)\t0.23749314013993011\n",
      "  (0, 29)\t0.23749314013993011\n",
      "  (0, 55)\t0.23749314013993011\n",
      "  (0, 25)\t0.23749314013993011\n",
      "  (0, 16)\t0.23749314013993011\n",
      "  (0, 0)\t0.23749314013993011\n",
      "  (0, 1)\t0.23749314013993011\n",
      "  (0, 15)\t0.23749314013993011\n",
      "  (1, 44)\t0.29031547855150247\n",
      "  (1, 5)\t0.29031547855150247\n",
      "  (1, 28)\t0.29031547855150247\n",
      "  (1, 53)\t0.22888805789011155\n",
      "  (1, 48)\t0.29031547855150247\n",
      "  (1, 58)\t0.29031547855150247\n",
      "  (1, 21)\t0.29031547855150247\n",
      "  (1, 9)\t0.29031547855150247\n",
      "  :\t:\n",
      "  (2, 11)\t0.18990156414931925\n",
      "  (2, 39)\t0.18990156414931925\n",
      "  (2, 59)\t0.18990156414931925\n",
      "  (2, 8)\t0.18990156414931925\n",
      "  (2, 34)\t0.18990156414931925\n",
      "  (2, 40)\t0.18990156414931925\n",
      "  (2, 45)\t0.18990156414931925\n",
      "  (2, 50)\t0.18990156414931925\n",
      "  (2, 35)\t0.18990156414931925\n",
      "  (2, 31)\t0.18990156414931925\n",
      "  (2, 46)\t0.18990156414931925\n",
      "  (2, 12)\t0.18990156414931925\n",
      "  (2, 2)\t0.18990156414931925\n",
      "  (3, 52)\t0.23918972200786026\n",
      "  (3, 54)\t0.23918972200786026\n",
      "  (3, 13)\t0.23918972200786026\n",
      "  (3, 38)\t0.30338183323942114\n",
      "  (3, 57)\t0.30338183323942114\n",
      "  (3, 3)\t0.30338183323942114\n",
      "  (3, 22)\t0.30338183323942114\n",
      "  (3, 41)\t0.30338183323942114\n",
      "  (3, 18)\t0.30338183323942114\n",
      "  (3, 33)\t0.30338183323942114\n",
      "  (3, 26)\t0.30338183323942114\n",
      "  (3, 42)\t0.30338183323942114\n",
      "['10', '20', 'after', 'again', 'all', 'am', 'amount', 'an', 'and', 'any', 'appreciated', 'ass', 'bad', 'be', 'better', 'chill', 'com', 'completely', 'damn', 'do', 'document', 'donation', 'ever', 'fine', 'first', 'google', 'got', 'greatly', 'happy', 'https', 'is', 'isn', 'it', 'just', 'laugh', 'life', 'make', 'need', 'never', 'of', 'off', 'oh', 'older', 'or', 'playing', 'realise', 'so', 'sometimes', 'take', 'test', 'that', 'the', 'this', 'to', 'will', 'www', 'you', 'young', 'your', 'yourself']\n"
     ]
    }
   ],
   "source": [
    "# Testing word count\n",
    "vectorizer = TfidfVectorizer()\n",
    "print(vectorizer.fit_transform(corpus))\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
